{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FGqy4q5lN21e",
        "GPg25WfUI36t",
        "n5tCkSfIJ5Vj"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gym Style toolkit\n"
      ],
      "metadata": {
        "id": "FGqy4q5lN21e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cYBUQ_rVkflY"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Static Solver"
      ],
      "metadata": {
        "id": "GPg25WfUI36t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "! pip install pulp\n",
        "import pulp\n",
        "from pulp import value\n",
        "\n",
        "class StaticSolver:\n",
        "    def __init__(self, matches: np.ndarray, arrival_rates: np.ndarray, rewards: np.ndarray):\n",
        "        \"\"\"Initialize static matching model.\n",
        "\n",
        "        Args:\n",
        "            matches: Match matrix (d x n) where d is num match types, n is num agent types\n",
        "            arrival_rates: Arrival rates vector of length n\n",
        "            rewards: Rewards vector of length d\n",
        "        \"\"\"\n",
        "        self.matches = matches\n",
        "        self.d = matches.shape[0]  # number of match types\n",
        "        self.n = matches.shape[1]  # number of agent types\n",
        "\n",
        "        # Create optimization model\n",
        "        self.model = pulp.LpProblem(\"StaticMatching\", pulp.LpMaximize)\n",
        "\n",
        "        # Decision variables\n",
        "        self.x = [pulp.LpVariable(f'x_{i}', lowBound=0) for i in range(self.d)]\n",
        "\n",
        "        # Objective\n",
        "        self.model += pulp.lpSum(rewards[i] * self.x[i] for i in range(self.d))\n",
        "\n",
        "        # Constraints\n",
        "        for j in range(self.n):\n",
        "            self.model += (\n",
        "                pulp.lpSum(matches[i,j] * self.x[i] for i in range(self.d)) == arrival_rates[j],\n",
        "                f'flow_conservation_{j}'\n",
        "            )\n",
        "\n",
        "    def solve(self, use_gurobi: bool = True) -> None:\n",
        "        \"\"\"Solve the optimization problem.\n",
        "\n",
        "        Args:\n",
        "            use_gurobi: If True, use Gurobi solver; otherwise use CBC (default False)\n",
        "        \"\"\"\n",
        "        if use_gurobi:\n",
        "            try:\n",
        "                from pulp import GUROBI\n",
        "                try:\n",
        "                    self.model.solve(GUROBI(msg=0))\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Gurobi solver failed: {e}, falling back to CBC solver\")\n",
        "                    self.model.solve(pulp.PULP_CBC_CMD(msg=0))\n",
        "            except ImportError:\n",
        "                print(\"Warning: Gurobi not available, falling back to CBC solver\")\n",
        "                self.model.solve(pulp.PULP_CBC_CMD(msg=0))\n",
        "        else:\n",
        "            self.model.solve(pulp.PULP_CBC_CMD(msg=0))\n",
        "\n",
        "    def get_primal_solution(self) -> np.ndarray:\n",
        "        \"\"\"Get primal solution (matching rates).\"\"\"\n",
        "        if self.model.status != pulp.LpStatusOptimal:\n",
        "            raise ValueError(f\"Model not solved optimally. Status: {pulp.LpStatus[self.model.status]}\")\n",
        "        return np.array([v.varValue for v in self.x])\n",
        "\n",
        "    def get_dual_solution(self) -> np.ndarray:\n",
        "        \"\"\"Get dual solution (shadow prices).\"\"\"\n",
        "        if self.model.status != pulp.LpStatusOptimal:\n",
        "            raise ValueError(f\"Model not solved optimally. Status: {pulp.LpStatus[self.model.status]}\")\n",
        "        return np.array([self.model.constraints[f'flow_conservation_{j}'].pi for j in range(self.n)])\n",
        "\n",
        "    def update_arrival_rates(self, new_rates: np.ndarray) -> None:\n",
        "        \"\"\"Update arrival rates in the model.\"\"\"\n",
        "        for j in range(self.n):\n",
        "            self.model.constraints[f'flow_conservation_{j}'].changeRHS(new_rates[j])\n",
        "        # Reset solution status\n",
        "        self.model.status = pulp.LpStatusNotSolved\n",
        "\n",
        "    def opt_obj(self):\n",
        "        return value(self.model.objective)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiQu-TzFJALB",
        "outputId": "8cc6d31e-d50d-4576-a490-8a2fa7fbd61b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pulp in /usr/local/lib/python3.12/dist-packages (3.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config of the Env"
      ],
      "metadata": {
        "id": "DEuR5rh2IvLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The config of the environment of Dynamic Matching problem is defined as follows:\n",
        "\n",
        "Config = {\n",
        "    T: int, # number of time periods\n",
        "    AR: np.ndarry, # Arrival Rate -> shape: (T, d)\n",
        "    M: np.ndarry, # Mathces -> shape: (m, d)\n",
        "    R: np.ndarry, # Reward -> shape: (T, m)\n",
        "    IQ: np.ndarry, # Initial Queue -> shape: (d)\n",
        "    r: float, # Discount Rate: float\n",
        "    a: float  # Abandon Rate\n",
        "}\n",
        "\n",
        "where T is the number of time periods, m is the number of match types, d is the number of agent types.\n",
        "\"\"\"\n",
        "\n",
        "matches = np.array([\n",
        "    [1,0,0,0,0,0,0,0],\n",
        "    [0,1,0,0,0,0,0,0],\n",
        "    [0,0,1,0,0,0,0,0],\n",
        "    [0,0,0,1,0,0,0,0],\n",
        "    [0,0,0,0,1,0,0,0],\n",
        "    [0,0,0,0,0,1,0,0],\n",
        "    [0,0,0,0,0,0,1,0],\n",
        "    [0,0,0,0,0,0,0,1],\n",
        "    [1,0,0,0,0,1,0,0],\n",
        "    [1,0,0,0,0,0,1,0],\n",
        "    [1,0,0,0,0,0,0,1],\n",
        "    [0,1,0,0,0,0,0,1],\n",
        "    [0,0,1,0,1,0,0,0],\n",
        "    [0,0,1,0,0,1,0,0],\n",
        "    [0,0,1,0,0,0,0,1],\n",
        "    [0,0,0,1,1,0,0,0],\n",
        "])\n",
        "\n",
        "rewards = [0., 0., 0., 0., 0., 0.,\n",
        "           0., 0., 0.99530526, 1.0054256,\n",
        "           0.99536582, 0.9953427, 1.00241962,\n",
        "           0.9808672, 0.98275082, 0.99437712]\n",
        "\n",
        "arrival_rates = np.array([\n",
        "    0.12493259, 0.12414321, 0.12512027, 0.12620845,\n",
        "    0.12402401, 0.12402403, 0.1262783, 0.12526913\n",
        "])\n",
        "\n",
        "arrival = np.array([0, 0, 0, 0, 0, 0, 1, 0])\n",
        "\n",
        "\n",
        "DM_config ={\n",
        "    \"T\": 10,\n",
        "    \"AR\": np.tile(arrival_rates, (10, 1)),\n",
        "    \"M\": matches,\n",
        "    \"R\": np.tile(rewards, (10, 1)),\n",
        "    'IQ': arrival,\n",
        "    \"r\": 1.,\n",
        "    \"a\": 0.\n",
        "}\n",
        "\n",
        "print(\"Matches shape:\", matches.shape)\n",
        "print(\"\\nExample matches:\")\n",
        "print(matches)\n",
        "print(\"\\nRewards:\", rewards)\n",
        "print(\"\\nArrival rates:\", arrival_rates)\n"
      ],
      "metadata": {
        "id": "EhJVP_ZXD4_Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18c9628b-3fac-4517-83d9-ed352a88ccbd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matches shape: (16, 8)\n",
            "\n",
            "Example matches:\n",
            "[[1 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 1]\n",
            " [1 0 0 0 0 1 0 0]\n",
            " [1 0 0 0 0 0 1 0]\n",
            " [1 0 0 0 0 0 0 1]\n",
            " [0 1 0 0 0 0 0 1]\n",
            " [0 0 1 0 1 0 0 0]\n",
            " [0 0 1 0 0 1 0 0]\n",
            " [0 0 1 0 0 0 0 1]\n",
            " [0 0 0 1 1 0 0 0]]\n",
            "\n",
            "Rewards: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99530526, 1.0054256, 0.99536582, 0.9953427, 1.00241962, 0.9808672, 0.98275082, 0.99437712]\n",
            "\n",
            "Arrival rates: [0.12493259 0.12414321 0.12512027 0.12620845 0.12402401 0.12402403\n",
            " 0.1262783  0.12526913]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creat Env"
      ],
      "metadata": {
        "id": "n5tCkSfIJ5Vj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DynamicMatching(gym.Env):\n",
        "    \"\"\"\n",
        "    An environment representing the dynamic mathcing problem\n",
        "\n",
        "    Attributes:\n",
        "        T: int, # number of time periods\n",
        "        AR: np.ndarry, # Arrival Rate -> shape: (T, d)\n",
        "        M: np.ndarry, # Mathces -> shape: (m, d)\n",
        "        R: np.ndarry, # Reward -> shape: (T, m)\n",
        "        PQ: np.ndarry, # Pysical Queue -> shape: (d,)\n",
        "        VQ: np.ndarry, # Virtual Queue -> shape: (d,)\n",
        "        r: float, # Discount Rate: float\n",
        "        a: float  # Abandon Rate\n",
        "\n",
        "    where T is the number of time periods, m is the number of match types, d is the number of agent types.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "\n",
        "        # Intializes Env parameters based on configuration dictionary\n",
        "        self.config = config\n",
        "        self.T = config[\"T\"]\n",
        "        self.AR = config[\"AR\"]\n",
        "        self.M = config[\"M\"]\n",
        "        self.R = config[\"R\"]\n",
        "        self.r = config[\"r\"]\n",
        "        self.a = config[\"a\"]\n",
        "\n",
        "        self.m = self.M.shape[0] # m is the number of match types\n",
        "        self.d = self.M.shape[1] # d is the number of agent types\n",
        "        self.seed = 16\n",
        "\n",
        "        self.action_space = gym.spaces.Discrete(self.m, seed=self.seed)\n",
        "        self.observation_space = gym.spaces.Discrete(self.d, seed=self.seed)\n",
        "        self.initial_queue = config[\"IQ\"].copy()\n",
        "        self.physical_queue = self.initial_queue.copy()\n",
        "        self.virtual_queue = self.initial_queue.copy()\n",
        "\n",
        "        self.timestep = 0\n",
        "        self.reset()\n",
        "\n",
        "    def get_config(self):\n",
        "        return self.config\n",
        "\n",
        "    # Reset the environemnt to initial state\n",
        "    def reset(self):\n",
        "        self.timestep = 0\n",
        "        self.physical_queue = self.initial_queue.copy()\n",
        "        self.virtual_queue = self.initial_queue.copy()\n",
        "        return\n",
        "\n",
        "    # Defines one step of the DM, returning the new state, reward, whether time horizon is finished and unrealized action list\n",
        "    def step(self, action_list, update=True):\n",
        "\n",
        "\n",
        "        if self.timestep >= self.T:\n",
        "            raise ValueError(\"Time horizon is finished\") # Check the time horizon\n",
        "        for i, action in enumerate(action_list):\n",
        "            assert self.action_space.contains(action) or (action == None), f\"Invalid action at index {i}: {action}\" # Check the Action List is Valid\n",
        "\n",
        "        # Update the Physical Queue\n",
        "        # Realize the Action in the Action list.\n",
        "        physical_rewards = 0\n",
        "        actions_to_remove = []\n",
        "        for action in action_list:\n",
        "            if action == None:\n",
        "                continue\n",
        "            elif np.all(self.physical_queue >= self.M[action]):\n",
        "                self.physical_queue = self.physical_queue - self.M[action]\n",
        "                physical_rewards = physical_rewards + self.R[self.timestep, action]\n",
        "                actions_to_remove.append(action)\n",
        "        physical_rewards *= self.r ** (self.timestep) # Consider the discount rate at timestep t: reward * (r**t)\n",
        "\n",
        "        # Remove the realized actions\n",
        "        for action in actions_to_remove:\n",
        "            action_list.remove(action)\n",
        "\n",
        "        # Update the Virtual Queue\n",
        "        # Do not need to realize the action list\n",
        "        virtual_rewards = 0\n",
        "        for action in action_list:\n",
        "            if action == None:\n",
        "                continue\n",
        "            else:\n",
        "                self.virtual_queue = self.virtual_queue - self.M[action]\n",
        "                virtual_rewards = virtual_rewards + self.R[self.timestep, action]\n",
        "        virtual_rewards *= self.r ** (self.timestep) # Consider the discount rate at timestep t: reward * (r**t)\n",
        "\n",
        "\n",
        "\n",
        "        # Move to next period\n",
        "        episode_over = False\n",
        "        new_arrival = np.zeros(self.d)\n",
        "        if update == True:\n",
        "            self.timestep += 1\n",
        "            if self.timestep == self.T:\n",
        "                episode_over = True\n",
        "            else:\n",
        "                new_arrival_idx = self.observation_space.sample(probability=self.AR[self.timestep])\n",
        "                new_arrival[int(new_arrival_idx)] = 1\n",
        "                self.physical_queue = self.physical_queue + new_arrival\n",
        "                self.virtual_queue = self.virtual_queue + new_arrival\n",
        "\n",
        "\n",
        "        return [self.physical_queue.copy(), self.virtual_queue.copy()], physical_rewards, virtual_rewards, episode_over, new_arrival.copy(), action_list\n",
        "\n"
      ],
      "metadata": {
        "id": "8ycd6j5HJ9fg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ],
      "metadata": {
        "id": "6KTubHJUguYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Experiment(object):\n",
        "    \"\"\"Optional instrumentation for running an experiment.\n",
        "\n",
        "    Runs a simulation between an arbitrary openAI Gym environment and an algorithm, saving a dataset of (reward, time, space) complexity across each episode,\n",
        "    and optionally saves trajectory information.\n",
        "\n",
        "    Attributes:\n",
        "        seed: random seed set to allow reproducibility\n",
        "        dirPath: (string) location to store the data files\n",
        "        nEps: (int) number of episodes for the simulation\n",
        "        deBug: (bool) boolean, when set to true causes the algorithm to print information to the command line\n",
        "        env: (openAI env) the environment to run the simulations on\n",
        "        epLen: (int) the length of each episode\n",
        "        numIters: (int) the number of iterations of (nEps, epLen) pairs to iterate over with the environment\n",
        "        save_trajectory: (bool) boolean, when set to true saves the entire trajectory information\n",
        "        render_flag: (bool) boolean, when set to true renders the simulations\n",
        "        agent: (or_suite.agent.Agent) an algorithm to run the experiments with\n",
        "        data: (np.array) an array saving the metrics along the sample paths (rewards, time, space)\n",
        "        trajectory_data: (list) a list saving the trajectory information\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env, agent):\n",
        "        '''\n",
        "        Args:\n",
        "            env: (openAI env) the environment to run the simulations on\n",
        "            agent: (or_suite.agent.Agent) an algorithm to run the experiments with\n",
        "            dict: a dictionary containing the arguments to send for the experiment, including:\n",
        "                dirPath: (string) location to store the data files\n",
        "                nEps: (int) number of episodes for the simulation\n",
        "                deBug: (bool) boolean, when set to true causes the algorithm to print information to the command line\n",
        "                env: (openAI env) the environment to run the simulations on\n",
        "                epLen: (int) the length of each episode\n",
        "                numIters: (int) the number of iterations of (nEps, epLen) pairs to iterate over with the environment\n",
        "                save_trajectory: (bool) boolean, when set to true saves the entire trajectory information\n",
        "                render: (bool) boolean, when set to true renders the simulations\n",
        "                pickle: (bool) when set to true saves data to a pickle file\n",
        "        '''\n",
        "\n",
        "        self.seed = 12\n",
        "        self.env = env\n",
        "        self.epLen = 10\n",
        "        self.num_iters = 1\n",
        "        self.agent = agent\n",
        "\n",
        "        np.random.seed(self.seed)  # sets seed for experiment\n",
        "\n",
        "    # Runs the experiment\n",
        "    def run(self):\n",
        "        '''\n",
        "            Runs the simulations between an environment and an algorithm\n",
        "        '''\n",
        "        for ite in range(self.num_iters):  # loops over the episodes\n",
        "\n",
        "            # Reset the environment\n",
        "            self.env.reset()\n",
        "\n",
        "            # Reset the agent\n",
        "            self.agent.reset()\n",
        "            self.agent.update_config(self.env, self.env.get_config())\n",
        "\n",
        "\n",
        "            oldState = self.env.initial_queue # obtains old state\n",
        "            arrival = self.env.initial_queue\n",
        "            epReward = 0\n",
        "\n",
        "\n",
        "            # repeats until episode is finished\n",
        "            for t in range(self.epLen):\n",
        "                print(\"\\n\"+\"=\"*10 + f\"{t}-th period\" + \"=\"*10)\n",
        "                # Select action list\n",
        "                action_list = self.agent.pick_action(\n",
        "                        queue=oldState, arrival=arrival,t=t)\n",
        "                print(f\"Proposed Action List: {action_list}\")\n",
        "\n",
        "                # steps based on the action chosen by the algorithm\n",
        "                queue, physical_rewards, virtual_rewards, episode_over, new_arrival, remain_action_list = self.env.step(action_list)\n",
        "                print(f\"Remain Action List: {remain_action_list}\")\n",
        "\n",
        "                epReward += physical_rewards\n",
        "\n",
        "                oldState = queue[self.agent.virtual]\n",
        "                arrival = new_arrival\n",
        "\n",
        "                # Update the Policy\n",
        "                self.agent.update_policy(arrival=arrival, remain_action_list=remain_action_list, t=t)\n",
        "\n",
        "            print(f\"\\nTotal Rewards: {epReward}\")\n",
        "\n",
        "            self.env.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "j_kXt2n_gyjL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agents"
      ],
      "metadata": {
        "id": "m3qsZnvdkDmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "All agents should inherit from the Agent class.\n",
        "'''\n",
        "class Agent(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def reset(self):\n",
        "        pass\n",
        "    def update_config(self, env, config):\n",
        "        ''' Update agent information based on the config__file'''\n",
        "        pass\n",
        "    def update_parameters(self, param):\n",
        "        pass\n",
        "    def update_obs(self, obs, action, reward, newObs, timestep, info):\n",
        "        '''Add observation to records'''\n",
        "        pass\n",
        "    def update_policy(self, h):\n",
        "        '''Update internal policy based upon records'''\n",
        "        pass\n",
        "    def pick_action(self, obs, h):\n",
        "        '''Select an action based upon the observation'''\n",
        "        pass"
      ],
      "metadata": {
        "id": "CTO4_ID_kGRG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Greedy Agent"
      ],
      "metadata": {
        "id": "emysTAEapGKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GreedyAgent(Agent):\n",
        "    def __init__(self):\n",
        "        self.matches = None\n",
        "        self.m = None\n",
        "        self.rewards = None\n",
        "        self.arrivals = None\n",
        "        self.virtual = False\n",
        "\n",
        "    def reset(self):\n",
        "        self.matches = None\n",
        "        self.m = None\n",
        "        self.rewards = None\n",
        "        self.arrivals = None\n",
        "        self.virtual = False\n",
        "\n",
        "    def update_config(self, env, config):\n",
        "        ''' Update agent information based on the config__file'''\n",
        "        self.matches = config[\"M\"]\n",
        "        self.m = self.matches.shape[0]\n",
        "        self.rewards = config[\"R\"]\n",
        "        self.arrivals = config[\"IQ\"] # Inital Queue\n",
        "\n",
        "    def update_parameters(self, param):\n",
        "        pass\n",
        "\n",
        "    def update_obs(self, obs, action, reward, newObs, timestep, info):\n",
        "        '''Add observation to records'''\n",
        "        pass\n",
        "\n",
        "    def update_policy(self, arrival=None, remain_action_list=None, t=None):\n",
        "        '''Update internal policy based upon records'''\n",
        "        pass\n",
        "\n",
        "    def pick_action(self, queue, arrival, t):\n",
        "        '''Select an action based upon the observation'''\n",
        "        best_reward = 0\n",
        "        best_match_idx = None\n",
        "\n",
        "        for i in range(self.m):\n",
        "            # Check if match uses current arrival\n",
        "            if np.inner(self.matches[i], arrival) > 1e-5:\n",
        "                # Check if we have enough agents\n",
        "                if np.all(queue >= self.matches[i]) and self.rewards[t][i] > best_reward:\n",
        "                    best_match_idx = i\n",
        "                    best_reward = self.rewards[t][i]\n",
        "        return [best_match_idx]\n",
        "\n"
      ],
      "metadata": {
        "id": "NpwGW_YfoqAY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DM_ENV = DynamicMatching(DM_config)\n",
        "DM_AGENT = GreedyAgent()\n",
        "\n",
        "Greedy_Exp = Experiment(env = DM_ENV, agent = DM_AGENT)\n",
        "Greedy_Exp.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONE6CPvEuCZL",
        "outputId": "e3909e49-cc1d-4de9-f4ad-08833635be2c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==========0-th period==========\n",
            "Proposed Action List: [None]\n",
            "Remain Action List: [None]\n",
            "\n",
            "==========1-th period==========\n",
            "Proposed Action List: [None]\n",
            "Remain Action List: [None]\n",
            "\n",
            "==========2-th period==========\n",
            "Proposed Action List: [15]\n",
            "Remain Action List: []\n",
            "\n",
            "==========3-th period==========\n",
            "Proposed Action List: [9]\n",
            "Remain Action List: []\n",
            "\n",
            "==========4-th period==========\n",
            "Proposed Action List: [None]\n",
            "Remain Action List: [None]\n",
            "\n",
            "==========5-th period==========\n",
            "Proposed Action List: [12]\n",
            "Remain Action List: []\n",
            "\n",
            "==========6-th period==========\n",
            "Proposed Action List: [None]\n",
            "Remain Action List: [None]\n",
            "\n",
            "==========7-th period==========\n",
            "Proposed Action List: [9]\n",
            "Remain Action List: []\n",
            "\n",
            "==========8-th period==========\n",
            "Proposed Action List: [None]\n",
            "Remain Action List: [None]\n",
            "\n",
            "==========9-th period==========\n",
            "Proposed Action List: [9]\n",
            "Remain Action List: []\n",
            "\n",
            "Total Rewards: 5.01307354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MaxQueue Agent"
      ],
      "metadata": {
        "id": "T-Hk3AOZ5Ror"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxQueueAgent(Agent):\n",
        "    def __init__(self):\n",
        "        self.matches = None\n",
        "        self.m = None\n",
        "        self.rewards = None\n",
        "        self.arrivals = None\n",
        "        self.arrival_rates = None\n",
        "        self.virtual = False\n",
        "        self.valid_indices = None\n",
        "\n",
        "    def reset(self):\n",
        "        self.matches = None\n",
        "        self.m = None\n",
        "        self.rewards = None\n",
        "        self.arrivals = None\n",
        "        self.arrival_rates = None\n",
        "        self.virtual = False\n",
        "        self.valid_indices = None\n",
        "\n",
        "    def update_config(self, env, config):\n",
        "        ''' Update agent information based on the config__file'''\n",
        "        self.matches = config[\"M\"]\n",
        "        self.m = self.matches.shape[0]\n",
        "        self.rewards = config[\"R\"]\n",
        "        self.arrivals = config[\"IQ\"] # Inital Queue\n",
        "        self.arrival_rates = config[\"AR\"] # arrival rates\n",
        "\n",
        "        # Get optimal offline solution\n",
        "        solver = StaticSolver(self.matches, self.arrival_rates[0], self.rewards[0])\n",
        "        solver.solve()  # Will use gurobi if available\n",
        "        primal_soln = solver.get_primal_solution()\n",
        "\n",
        "        # Get valid matches (positive in optimal solution)\n",
        "        self.valid_indices = np.where(primal_soln > 1e-10)[0]\n",
        "        print(f\"Valid indices: {self.valid_indices}\")\n",
        "\n",
        "        # Validate solution\n",
        "        if len(self.valid_indices) != self.matches.shape[1]:\n",
        "            raise ValueError(\"Primal solution not basic feasible\")\n",
        "\n",
        "\n",
        "    def update_parameters(self, param):\n",
        "        pass\n",
        "\n",
        "    def update_obs(self, obs, action, reward, newObs, timestep, info):\n",
        "        '''Add observation to records'''\n",
        "        pass\n",
        "\n",
        "    def update_policy(self, arrival=None, remain_action_list=None, t=None):\n",
        "        '''Update internal policy based upon records'''\n",
        "        pass\n",
        "\n",
        "    def pick_action(self, queue, arrival, t):\n",
        "        '''Select an action based upon the observation'''\n",
        "        best_match_idx = None\n",
        "        highest_sum = 0\n",
        "\n",
        "        for i in self.valid_indices:\n",
        "            if np.all(queue >= self.matches[i]):\n",
        "                queue_sum = np.inner(self.matches[i], queue)\n",
        "                if queue_sum > highest_sum:\n",
        "                    best_match_idx = i\n",
        "                    highest_sum = queue_sum\n",
        "\n",
        "        return [best_match_idx]\n",
        "\n"
      ],
      "metadata": {
        "id": "6i3KzJSm5f6g"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DM_ENV = DynamicMatching(DM_config)\n",
        "DM_AGENT = MaxQueueAgent()\n",
        "\n",
        "Greey_Exp = Experiment(env = DM_ENV, agent = DM_AGENT)\n",
        "Greey_Exp.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eg7PrMrG9LFM",
        "outputId": "eaca7884-b28d-44d9-a355-ac9334e0e2c5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Gurobi solver failed: GUROBI: Not Available, falling back to CBC solver\n",
            "Valid indices: [ 3  5  6  9 11 13 14 15]\n",
            "\n",
            "==========0-th period==========\n",
            "Proposed Action List: [np.int64(6)]\n",
            "Remain Action List: []\n",
            "\n",
            "==========1-th period==========\n",
            "Proposed Action List: [None]\n",
            "Remain Action List: [None]\n",
            "\n",
            "==========2-th period==========\n",
            "Proposed Action List: [np.int64(15)]\n",
            "Remain Action List: []\n",
            "\n",
            "==========3-th period==========\n",
            "Proposed Action List: [None]\n",
            "Remain Action List: [None]\n",
            "\n",
            "==========4-th period==========\n",
            "Proposed Action List: [None]\n",
            "Remain Action List: [None]\n",
            "\n",
            "==========5-th period==========\n",
            "Proposed Action List: [None]\n",
            "Remain Action List: [None]\n",
            "\n",
            "==========6-th period==========\n",
            "Proposed Action List: [None]\n",
            "Remain Action List: [None]\n",
            "\n",
            "==========7-th period==========\n",
            "Proposed Action List: [np.int64(9)]\n",
            "Remain Action List: []\n",
            "\n",
            "==========8-th period==========\n",
            "Proposed Action List: [np.int64(9)]\n",
            "Remain Action List: []\n",
            "\n",
            "==========9-th period==========\n",
            "Proposed Action List: [None]\n",
            "Remain Action List: [None]\n",
            "\n",
            "Total Rewards: 3.0052283199999996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primal-Dual Blind Agent"
      ],
      "metadata": {
        "id": "XHg7MlVLVc6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PrimalDualBlindAgent(Agent):\n",
        "    def __init__(self):\n",
        "        self.T = None\n",
        "        self.matches = None\n",
        "        self.m = None\n",
        "        self.rewards = None\n",
        "        self.arrivals = None\n",
        "        self.arrival_rates = None\n",
        "        self.arrival_sum = None\n",
        "        self.virtual = True # Using Virtual Queue as State\n",
        "        self.valid_indices = None\n",
        "        self.unrealized_matches = []\n",
        "        self.v_vec = None\n",
        "        self.solver = None\n",
        "\n",
        "    def reset(self):\n",
        "        self.T = None\n",
        "        self.matches = None\n",
        "        self.m = None\n",
        "        self.rewards = None\n",
        "        self.arrivals = None\n",
        "        self.arrival_rates = None\n",
        "        self.arrival_sum = None\n",
        "        self.virtual = True # Using Virtual Queue as State\n",
        "        self.valid_indices = None\n",
        "        self.unrealized_matches = []\n",
        "        self.v_vec = None\n",
        "        self.solver = None\n",
        "\n",
        "    def update_config(self, env, config):\n",
        "        ''' Update agent information based on the config__file'''\n",
        "        self.T = config[\"T\"]\n",
        "        self.matches = config[\"M\"]\n",
        "        self.m = self.matches.shape[0]\n",
        "        self.rewards = config[\"R\"]\n",
        "        self.arrivals = config[\"IQ\"] # Inital Queue\n",
        "        self.arrival_rates = config[\"AR\"] # arrival rates\n",
        "        self.arrival_sum = config[\"IQ\"]\n",
        "        self.v_vec = [100.0] * self.T\n",
        "\n",
        "        # Initialize solver with empirical arrival rates\n",
        "        self.solver = StaticSolver(self.matches, self.arrival_sum, self.rewards[0])\n",
        "\n",
        "\n",
        "\n",
        "    def update_parameters(self, param):\n",
        "        pass\n",
        "\n",
        "    def update_obs(self, obs, action, reward, newObs, timestep, info):\n",
        "        '''Add observation to records'''\n",
        "        pass\n",
        "\n",
        "    def update_policy(self, arrival=None, remain_action_list=None, t=None):\n",
        "        '''Update internal policy based upon records'''\n",
        "\n",
        "        # Update empirical arrival rates with a minimum threshold to prevent numerical issues\n",
        "        self.arrival_sum = self.arrival_sum + arrival\n",
        "        empirical_rates = self.arrival_sum / (t + 2)\n",
        "        self.solver.update_arrival_rates(empirical_rates)\n",
        "\n",
        "        self.unrealized_matches = remain_action_list\n",
        "\n",
        "\n",
        "    def pick_action(self, queue, arrival, t):\n",
        "        '''Select an action based upon the observation'''\n",
        "\n",
        "\n",
        "        # Get empirical dual values\n",
        "        self.solver.solve()\n",
        "        dual_values = self.solver.get_dual_solution()\n",
        "\n",
        "        # Find best match based on reduced reward\n",
        "        best_match_idx = None\n",
        "        highest_reduced_reward = 1e-6\n",
        "\n",
        "        for m in range(self.m):\n",
        "            # Only consider matches using current arrival\n",
        "            if np.inner(self.matches[m], arrival) > 0:\n",
        "                # Compute reduced reward with queue pressure\n",
        "                reduced_reward = (\n",
        "                    self.rewards[t][m] -\n",
        "                    np.inner(dual_values, self.matches[m]) +\n",
        "                    np.inner(queue, self.matches[m]) / self.v_vec[t]\n",
        "                )\n",
        "                if reduced_reward > highest_reduced_reward:\n",
        "                    best_match_idx = m\n",
        "                    highest_reduced_reward = reduced_reward\n",
        "\n",
        "        self.unrealized_matches.append(best_match_idx)\n",
        "\n",
        "        return self.unrealized_matches\n",
        "\n"
      ],
      "metadata": {
        "id": "EL6-UJpiVj_F"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DM_ENV = DynamicMatching(DM_config)\n",
        "DM_AGENT = PrimalDualBlindAgent()\n",
        "\n",
        "PrimalDual_Blind_Exp = Experiment(env = DM_ENV, agent = DM_AGENT)\n",
        "PrimalDual_Blind_Exp.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EFyMgV8bISN",
        "outputId": "c21e28c4-4564-4534-edf4-77350a7d24ca"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==========0-th period==========\n",
            "Warning: Gurobi solver failed: GUROBI: Not Available, falling back to CBC solver\n",
            "Proposed Action List: [6]\n",
            "Remain Action List: []\n",
            "\n",
            "==========1-th period==========\n",
            "Warning: Gurobi solver failed: GUROBI: Not Available, falling back to CBC solver\n",
            "Proposed Action List: [12]\n",
            "Remain Action List: [12]\n",
            "\n",
            "==========2-th period==========\n",
            "Warning: Gurobi solver failed: GUROBI: Not Available, falling back to CBC solver\n",
            "Proposed Action List: [12, 15]\n",
            "Remain Action List: [12]\n",
            "\n",
            "==========3-th period==========\n",
            "Warning: Gurobi solver failed: GUROBI: Not Available, falling back to CBC solver\n",
            "Proposed Action List: [12, 9]\n",
            "Remain Action List: [12, 9]\n",
            "\n",
            "==========4-th period==========\n",
            "Warning: Gurobi solver failed: GUROBI: Not Available, falling back to CBC solver\n",
            "Proposed Action List: [12, 9, None]\n",
            "Remain Action List: [12, 9, None]\n",
            "\n",
            "==========5-th period==========\n",
            "Warning: Gurobi solver failed: GUROBI: Not Available, falling back to CBC solver\n",
            "Proposed Action List: [12, 9, None, None]\n",
            "Remain Action List: [9, None, None]\n",
            "\n",
            "==========6-th period==========\n",
            "Warning: Gurobi solver failed: GUROBI: Not Available, falling back to CBC solver\n",
            "Proposed Action List: [9, None, None, None]\n",
            "Remain Action List: [9, None, None, None]\n",
            "\n",
            "==========7-th period==========\n",
            "Warning: Gurobi solver failed: GUROBI: Not Available, falling back to CBC solver\n",
            "Proposed Action List: [9, None, None, None, None]\n",
            "Remain Action List: [None, None, None, None]\n",
            "\n",
            "==========8-th period==========\n",
            "Warning: Gurobi solver failed: GUROBI: Not Available, falling back to CBC solver\n",
            "Proposed Action List: [None, None, None, None, None]\n",
            "Remain Action List: [None, None, None, None, None]\n",
            "\n",
            "==========9-th period==========\n",
            "Warning: Gurobi solver failed: GUROBI: Not Available, falling back to CBC solver\n",
            "Proposed Action List: [None, None, None, None, None, None]\n",
            "Remain Action List: [None, None, None, None, None, None]\n",
            "\n",
            "Total Rewards: 3.00222234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PrimalDualAgent"
      ],
      "metadata": {
        "id": "GtAXyo3tbzo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PrimalDualAgent(Agent):\n",
        "    def __init__(self):\n",
        "        self.T = None\n",
        "        self.matches = None\n",
        "        self.m = None\n",
        "        self.rewards = None\n",
        "        self.arrivals = None\n",
        "        self.arrival_rates = None\n",
        "        self.virtual = True # Using Virtual Queue as State\n",
        "        self.unrealized_matches = []\n",
        "        self.v_vec = None\n",
        "        self.solver = None\n",
        "\n",
        "    def reset(self):\n",
        "        self.T = None\n",
        "        self.matches = None\n",
        "        self.m = None\n",
        "        self.rewards = None\n",
        "        self.arrivals = None\n",
        "        self.arrival_rates = None\n",
        "        self.virtual = True # Using Virtual Queue as State\n",
        "        self.unrealized_matches = []\n",
        "        self.v_vec = None\n",
        "        self.solver = None\n",
        "\n",
        "    def update_config(self, env, config):\n",
        "        ''' Update agent information based on the config__file'''\n",
        "        self.T = config[\"T\"]\n",
        "        self.matches = config[\"M\"]\n",
        "        self.m = self.matches.shape[0]\n",
        "        self.rewards = config[\"R\"]\n",
        "        self.arrivals = config[\"IQ\"] # Inital Queue\n",
        "        self.arrival_rates = config[\"AR\"] # arrival rates\n",
        "        self.arrival_sum = config[\"IQ\"]\n",
        "        self.v_vec = [100.0] * self.T\n",
        "\n",
        "        # Initialize solver with empirical arrival rates\n",
        "        self.solver = StaticSolver(self.matches, self.arrival_rates[0], self.rewards[0])\n",
        "\n",
        "\n",
        "\n",
        "    def update_parameters(self, param):\n",
        "        pass\n",
        "\n",
        "    def update_obs(self, obs, action, reward, newObs, timestep, info):\n",
        "        '''Add observation to records'''\n",
        "        pass\n",
        "\n",
        "    def update_policy(self, arrival=None, remain_action_list=None, t=None):\n",
        "        '''Update internal policy based upon records'''\n",
        "        self.unrealized_matches = remain_action_list\n",
        "\n",
        "\n",
        "    def pick_action(self, queue, arrival, t):\n",
        "        '''Select an action based upon the observation'''\n",
        "\n",
        "\n",
        "        # Get empirical dual values\n",
        "        self.solver.solve()\n",
        "        dual_values = self.solver.get_dual_solution()\n",
        "\n",
        "        # Find best match based on reduced reward\n",
        "        best_match_idx = None\n",
        "        highest_reduced_reward = 1e-6\n",
        "\n",
        "        for m in range(self.m):\n",
        "            # Only consider matches using current arrival\n",
        "            if np.inner(self.matches[m], arrival) > 0:\n",
        "                # Compute reduced reward with queue pressure\n",
        "                reduced_reward = (\n",
        "                    self.rewards[t][m] -\n",
        "                    np.inner(dual_values, self.matches[m]) +\n",
        "                    np.inner(queue, self.matches[m]) / self.v_vec[t]\n",
        "                )\n",
        "                if reduced_reward > highest_reduced_reward:\n",
        "                    best_match_idx = m\n",
        "                    highest_reduced_reward = reduced_reward\n",
        "\n",
        "        self.unrealized_matches.append(best_match_idx)\n",
        "\n",
        "        return self.unrealized_matches\n",
        "\n"
      ],
      "metadata": {
        "id": "H-FbxBOUb3kd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DM_ENV = DynamicMatching(DM_config)\n",
        "DM_AGENT = PrimalDualAgent()\n",
        "\n",
        "PrimalDual_Exp = Experiment(env = DM_ENV, agent = DM_AGENT)\n",
        "PrimalDual_Exp.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvvjHe1Ribzm",
        "outputId": "eaf36e39-f34a-4086-9b2a-ef494c4964fe"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==========0-th period==========\n",
            "Warning: Gurobi solver failed: GUROBI: Not Available, falling back to CBC solver\n",
            "Proposed Action List: [6]\n",
            "Remain Action List: []\n",
            "\n",
            "==========1-th period==========\n",
            "Warning: Gurobi solver failed: GUROBI: Not Available, falling back to CBC solver\n",
            "Proposed Action List: [15]\n",
            "Remain Action List: [15]\n",
            "\n",
            "==========2-th period==========\n",
            "Warning: Gurobi solver failed: GUROBI: Not Available, falling back to CBC solver\n",
            "Proposed Action List: [15, None]\n",
            "Remain Action List: [None]\n",
            "\n",
            "==========3-th period==========\n",
            "Warning: Gurobi solver failed: GUROBI: Not Available, falling back to CBC solver\n",
            "Proposed Action List: [None, 9]\n",
            "Remain Action List: [None, 9]\n",
            "\n",
            "==========4-th period==========\n",
            "Warning: Gurobi solver failed: GUROBI: Not Available, falling back to CBC solver\n",
            "Proposed Action List: [None, 9, 13]\n",
            "Remain Action List: [None, 9, 13]\n",
            "\n",
            "==========5-th period==========\n",
            "Warning: Gurobi solver failed: GUROBI: Not Available, falling back to CBC solver\n",
            "Proposed Action List: [None, 9, 13, 15]\n",
            "Remain Action List: [None, 9, 13, 15]\n",
            "\n",
            "==========6-th period==========\n",
            "Warning: Gurobi solver failed: GUROBI: Not Available, falling back to CBC solver\n",
            "Proposed Action List: [None, 9, 13, 15, None]\n",
            "Remain Action List: [None, 9, 13, 15, None]\n",
            "\n",
            "==========7-th period==========\n",
            "Warning: Gurobi solver failed: GUROBI: Not Available, falling back to CBC solver\n",
            "Proposed Action List: [None, 9, 13, 15, None, None]\n",
            "Remain Action List: [None, 13, 15, None, None]\n",
            "\n",
            "==========8-th period==========\n",
            "Warning: Gurobi solver failed: GUROBI: Not Available, falling back to CBC solver\n",
            "Proposed Action List: [None, 13, 15, None, None, None]\n",
            "Remain Action List: [None, 13, 15, None, None, None]\n",
            "\n",
            "==========9-th period==========\n",
            "Warning: Gurobi solver failed: GUROBI: Not Available, falling back to CBC solver\n",
            "Proposed Action List: [None, 13, 15, None, None, None, None]\n",
            "Remain Action List: [None, 13, 15, None, None, None, None]\n",
            "\n",
            "Total Rewards: 1.99980272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kidney Exchange Simulator"
      ],
      "metadata": {
        "id": "KxQFarscrBAm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Packages"
      ],
      "metadata": {
        "id": "Mh2x9ubPjugS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import copy"
      ],
      "metadata": {
        "id": "MV1PiR4HbNro"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Basic data structures"
      ],
      "metadata": {
        "id": "lx_bdf2QbJ2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vertex:\n",
        "    def __init__(self, vid, is_patient, is_altruist, features, arrival_time=0):\n",
        "        self.id = vid\n",
        "        self.is_patient = is_patient\n",
        "        self.is_altruist = is_altruist\n",
        "        self.features = features\n",
        "        self.arrival_time = arrival_time\n",
        "\n",
        "\n",
        "class Graph:\n",
        "    def __init__(self):\n",
        "        self.V = {}  # {int: Vertex}\n",
        "        self.E = {}  # {(int, int): float}"
      ],
      "metadata": {
        "id": "HO66FfTPvpFA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matching (Naive Version)"
      ],
      "metadata": {
        "id": "g_IDMFZhbTip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from scipy.optimize import milp, LinearConstraint, Bounds\n",
        "\n",
        "def SolveIP(graph, max_cycle_len=3, max_chain_len=4):\n",
        "    # Build outgoing adjacency\n",
        "    out = collections.defaultdict(list)\n",
        "    for (u, v), w in graph.E.items():\n",
        "        if u in graph.V and v in graph.V:\n",
        "            out[u].append(v)\n",
        "\n",
        "    structures = []  # each: {\"nodes\": tuple, \"edges\": tuple[(u,v),...], \"w\": float}\n",
        "\n",
        "    # Enumerate cycles (pairs only)\n",
        "    pairs = [vid for vid, vv in graph.V.items() if not vv.is_altruist]\n",
        "    for start in pairs:\n",
        "        stack = [(start, [start])]\n",
        "        while stack:\n",
        "            cur, path = stack.pop()\n",
        "            if len(path) > max_cycle_len:\n",
        "                continue\n",
        "            for nxt in out.get(cur, []):\n",
        "                if nxt == start:\n",
        "                    # found a cycle\n",
        "                    if len(path) >= 2 and start == min(path):  # de-duplicate cycles\n",
        "                        cyc = path[:]\n",
        "                        edges = [(cyc[i], cyc[(i + 1) % len(cyc)]) for i in range(len(cyc))]\n",
        "                        if all(e in graph.E for e in edges):\n",
        "                            w = float(sum(graph.E[e] for e in edges))\n",
        "                            structures.append({\"nodes\": tuple(cyc), \"edges\": tuple(edges), \"w\": w})\n",
        "                else:\n",
        "                    if nxt in path:\n",
        "                        continue\n",
        "                    if nxt not in graph.V:\n",
        "                        continue\n",
        "                    if graph.V[nxt].is_altruist:\n",
        "                        continue\n",
        "                    stack.append((nxt, path + [nxt]))\n",
        "\n",
        "    #  Enumerate chains (altruist-start only)\n",
        "    altruists = [vid for vid, vv in graph.V.items() if vv.is_altruist]\n",
        "    for a in altruists:\n",
        "        stack = [(a, [a], [], 0.0)]  # (cur, nodes_path, edges_path, w)\n",
        "        while stack:\n",
        "            cur, nodes_path, edges_path, w = stack.pop()\n",
        "            if len(edges_path) >= max_chain_len:\n",
        "                continue\n",
        "            for nxt in out.get(cur, []):\n",
        "                if nxt not in graph.V:\n",
        "                    continue\n",
        "                if graph.V[nxt].is_altruist:\n",
        "                    continue\n",
        "                if nxt in nodes_path:\n",
        "                    continue\n",
        "                e = (cur, nxt)\n",
        "                if e not in graph.E:\n",
        "                    continue\n",
        "\n",
        "                new_nodes = nodes_path + [nxt]\n",
        "                new_edges = edges_path + [e]\n",
        "                new_w = w + float(graph.E[e])\n",
        "\n",
        "                # every prefix is a valid chain\n",
        "                structures.append({\"nodes\": tuple(new_nodes), \"edges\": tuple(new_edges), \"w\": float(new_w)})\n",
        "\n",
        "                # continue from nxt\n",
        "                stack.append((nxt, new_nodes, new_edges, new_w))\n",
        "\n",
        "    if not structures:\n",
        "        return []  # nothing feasible\n",
        "\n",
        "    # Build A matrix for Ax <= 1\n",
        "    vids = list(graph.V.keys())\n",
        "    vid_to_row = {vid: i for i, vid in enumerate(vids)}\n",
        "    n_rows = len(vids)\n",
        "    n_vars = len(structures)\n",
        "\n",
        "    rows, cols, data = [], [], []\n",
        "    for j, s in enumerate(structures):\n",
        "        for vid in set(s[\"nodes\"]):\n",
        "            rows.append(vid_to_row[vid])\n",
        "            cols.append(j)\n",
        "            data.append(1.0)\n",
        "\n",
        "    A = sp.csr_matrix((data, (rows, cols)), shape=(n_rows, n_vars))\n",
        "\n",
        "    lc = LinearConstraint(A, -np.inf * np.ones(n_rows), np.ones(n_rows)) # Linear constraints: Ax <= 1\n",
        "    bounds = Bounds(np.zeros(n_vars), np.ones(n_vars)) # bound decision vars:0 <= x <= 1\n",
        "    integrality = np.ones(n_vars, dtype=int)   # integer decision vars: {0, 1}\n",
        "\n",
        "    # Objective\n",
        "    c = -np.array([s[\"w\"] for s in structures], dtype=float)\n",
        "\n",
        "    res = milp(c=c, integrality=integrality, bounds=bounds, constraints=[lc]) # Utilize the Scipy Mixed-integer linear programming packages\n",
        "\n",
        "    # Check Slover Success\n",
        "    if res.x is None or res.status != 0:\n",
        "        msg = getattr(res, \"message\", \"MILP failed\")\n",
        "        raise RuntimeError(f\"SolveIP MILP failed (status={res.status}). {msg}\")\n",
        "\n",
        "    chosen = [structures[j] for j in range(n_vars) if res.x[j] > 0.5]\n",
        "\n",
        "    edges = []\n",
        "    for s in chosen:\n",
        "        edges.extend(list(s[\"edges\"]))\n",
        "    return edges\n"
      ],
      "metadata": {
        "id": "zn97WtdNSDyB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_matching(graph):\n",
        "    \"\"\"\n",
        "    Toy max-weight matching:\n",
        "      - sort edges by weight desc\n",
        "      - each vertex at most once as donor and once as patient\n",
        "    => The chosen edges form disjoint directed paths and directed cycles.\n",
        "    \"\"\"\n",
        "    edges_sorted = sorted(graph.E.items(), key=lambda kv: kv[1], reverse=True)\n",
        "\n",
        "    donors_used = set()\n",
        "    patients_used = set()\n",
        "    matching = []\n",
        "\n",
        "    for (u, v), w in edges_sorted:\n",
        "        if u in donors_used or v in patients_used:\n",
        "            continue\n",
        "        if u == v:\n",
        "            continue\n",
        "        matching.append((u, v))\n",
        "        donors_used.add(u)\n",
        "        patients_used.add(v)\n",
        "\n",
        "    return matching"
      ],
      "metadata": {
        "id": "e2G-UThnbgSC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Events (Expire, negative crossmatch and renege)"
      ],
      "metadata": {
        "id": "yVJni_NzdAA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def expire(vertex, rng, prob=0.01):\n",
        "    \"\"\"Paper: expire with (calibrated) constant probability. Here keep as parameter.\"\"\"\n",
        "    return rng.random() < prob\n",
        "\n",
        "\n",
        "def negative_crossmatch(patient_vertex, rng):\n",
        "    \"\"\"\n",
        "    Paper: failure probability depends on patient's CPRA.\n",
        "      P(fail) = CPRA/100\n",
        "      CPRA=100 -> fail prob = 1\n",
        "    \"\"\"\n",
        "    cpra = int(patient_vertex.features.get(\"cpra\", 0))\n",
        "    cpra = max(0, min(100, cpra))\n",
        "    return rng.random() < (cpra / 100.0)\n",
        "\n",
        "\n",
        "def renege(pair_vertex, rng, default_prob=0.02):\n",
        "    \"\"\"\n",
        "    Paper: only relevant for CHAINS (non-simultaneous)  the paired donor may renege\n",
        "    on continuing the chain.\n",
        "    We model it as a Bernoulli event whose probability can be a constant, or stored\n",
        "    per-vertex in features['renege_prob'].\n",
        "    \"\"\"\n",
        "    p = float(pair_vertex.features.get(\"renege_prob\", default_prob))\n",
        "    p = max(0.0, min(1.0, p))\n",
        "    return rng.random() < p"
      ],
      "metadata": {
        "id": "IOcQCZgVbrO6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Empirical samplers for f_p and f_a"
      ],
      "metadata": {
        "id": "DpgJ4Arac8HW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmpiricalSampler:\n",
        "    def __init__(self, bank, rng):\n",
        "        if not bank:\n",
        "            raise ValueError(\"bank is empty. Provide at least 1 record.\")\n",
        "        self.bank = bank\n",
        "        self.rng = rng\n",
        "\n",
        "    def __call__(self):\n",
        "        idx = int(self.rng.integers(0, len(self.bank)))\n",
        "        return copy.deepcopy(self.bank[idx])"
      ],
      "metadata": {
        "id": "ad25A2mvb08X"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ABOCompatible & w_OPTN"
      ],
      "metadata": {
        "id": "FRyggwfrc2je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def abo_compatible(donor_abo: str, cand_abo: str) -> bool:\n",
        "    '''\n",
        "    Blood Type Compatiable Funciton:\n",
        "        O compatible to A, B, AB, O\n",
        "        A compatible to A, AB\n",
        "        B compatible to B, AB\n",
        "        AB compatible to AB\n",
        "    '''\n",
        "    d = donor_abo.upper()\n",
        "    c = cand_abo.upper()\n",
        "    if d == \"O\":\n",
        "        return True\n",
        "    if d == \"A\":\n",
        "        return c in (\"A\", \"AB\")\n",
        "    if d == \"B\":\n",
        "        return c in (\"B\", \"AB\")\n",
        "    if d == \"AB\":\n",
        "        return c == \"AB\"\n",
        "    raise ValueError(f\"Unknown ABO: donor={donor_abo}, candidate={cand_abo}\")\n",
        "\n",
        "\n",
        "CAND_ABO_POINTS = {\"O\": 100, \"B\": 50, \"A\": 25, \"AB\": 0} # O only can recieve O type kidney -> higher priority\n",
        "PAIRED_DONOR_ABO_POINTS = {\"O\": 0, \"B\": 100, \"A\": 250, \"AB\": 500} # AB only can donor their kidney to AB -> higher priority\n",
        "\n",
        "\n",
        "def cpra_points(cpra: int) -> int:\n",
        "    cpra = int(cpra)\n",
        "    if not (0 <= cpra <= 100):\n",
        "        raise ValueError(\"CPRA must be in [0, 100].\")\n",
        "\n",
        "    if 0 <= cpra <= 19:  return 0\n",
        "    if 20 <= cpra <= 29: return 5\n",
        "    if 30 <= cpra <= 39: return 10\n",
        "    if 40 <= cpra <= 49: return 15\n",
        "    if 50 <= cpra <= 59: return 20\n",
        "    if 60 <= cpra <= 69: return 25\n",
        "    if 70 <= cpra <= 74: return 50\n",
        "    if 75 <= cpra <= 79: return 75\n",
        "    if 80 <= cpra <= 84: return 125\n",
        "    if 85 <= cpra <= 89: return 200\n",
        "    if 90 <= cpra <= 94: return 300\n",
        "    if cpra == 95: return 500\n",
        "    if cpra == 96: return 700\n",
        "    if cpra == 97: return 900\n",
        "    if cpra == 98: return 1250\n",
        "    if cpra == 99: return 1500\n",
        "    if cpra == 100: return 2000\n",
        "    raise RuntimeError(\"unreachable\")\n",
        "\n",
        "\n",
        "def _paired_donor_abo_points(paired_donor_abo):\n",
        "    \"\"\"If multiple ABO candidates exist, take the fewest points (conservative).\"\"\"\n",
        "    if paired_donor_abo is None:\n",
        "        return 0\n",
        "    if isinstance(paired_donor_abo, str):\n",
        "        return PAIRED_DONOR_ABO_POINTS[paired_donor_abo.upper()]\n",
        "    pts = [PAIRED_DONOR_ABO_POINTS[a.upper()] for a in paired_donor_abo]\n",
        "    return min(pts)\n",
        "\n",
        "\n",
        "def donor_abo_of(vertex: Vertex) -> str:\n",
        "    f = vertex.features\n",
        "    if vertex.is_altruist:\n",
        "        return f[\"donor_abo\"]\n",
        "    return f[\"paired_donor_abo\"]\n",
        "\n",
        "\n",
        "def candidate_abo_of(vertex: Vertex) -> str:\n",
        "    return vertex.features[\"candidate_abo\"]\n",
        "\n",
        "\n",
        "def w_optn(donor_vertex: Vertex, cand_vertex: Vertex) -> float:\n",
        "    f = cand_vertex.features\n",
        "    cand_abo = f[\"candidate_abo\"].upper()\n",
        "    cpra = int(f[\"cpra\"])\n",
        "    wait_days = int(f.get(\"wait_days\", 0))\n",
        "\n",
        "    w = 100.0 + 0.07 * max(0, wait_days)\n",
        "\n",
        "    # Check 0-ABDR mismatch\n",
        "    z = f.get(\"zero_abdr_mismatch\", False)\n",
        "    if isinstance(z, dict):\n",
        "        z = bool(z.get(donor_vertex.id, False))\n",
        "    if z:\n",
        "        w += 10.0\n",
        "\n",
        "    # Check the Same Hosipital\n",
        "    c1 = donor_vertex.features.get(\"center\", None)\n",
        "    c2 = f.get(\"center\", None)\n",
        "    if c1 is not None and c2 is not None and c1 == c2:\n",
        "        w += 75.0\n",
        "\n",
        "    # Check the Previous Cross Match record\n",
        "    px = f.get(\"prev_crossmatch_ok\", False)\n",
        "    if isinstance(px, dict):\n",
        "        px = bool(px.get(donor_vertex.id, False))\n",
        "    if px:\n",
        "        w += 75.0\n",
        "\n",
        "    # Check the Candidate's Age\n",
        "    age = f.get(\"candidate_age\", f.get(\"age\", None))\n",
        "    if age is not None and int(age) < 18:\n",
        "        w += 100.0\n",
        "\n",
        "    # Check the Candidate's donor history\n",
        "    if bool(f.get(\"prior_living_donor\", False)):\n",
        "        w += 150.0\n",
        "\n",
        "    w += float(CAND_ABO_POINTS[cand_abo])\n",
        "    w += float(_paired_donor_abo_points(f.get(\"paired_donor_abo\", None)))\n",
        "    w += float(cpra_points(cpra))\n",
        "\n",
        "    if bool(f.get(\"orphan\", False)):\n",
        "        w += 1000000.0\n",
        "\n",
        "    return w"
      ],
      "metadata": {
        "id": "W19IULtWb72P"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SetPool Function"
      ],
      "metadata": {
        "id": "RFth8Pi0cxB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_arrivals(t, graph, lam_p, lam_a, f_p, f_a, rng):\n",
        "    next_id = max(graph.V.keys()) + 1 if graph.V else 0\n",
        "    num_pairs = rng.poisson(lam_p)\n",
        "    num_altruists = rng.poisson(lam_a)\n",
        "\n",
        "    for _ in range(num_pairs):\n",
        "        vid = next_id\n",
        "        next_id += 1\n",
        "        graph.V[vid] = Vertex(vid, True, False, f_p(), arrival_time=t + 1)\n",
        "\n",
        "    for _ in range(num_altruists):\n",
        "        vid = next_id\n",
        "        next_id += 1\n",
        "        graph.V[vid] = Vertex(vid, False, True, f_a(), arrival_time=t + 1)\n",
        "\n",
        "\n",
        "def _ensure_dict_feature(cand_features: dict, key: str) -> dict:\n",
        "    if key not in cand_features or cand_features[key] is None:\n",
        "        cand_features[key] = {}\n",
        "    elif not isinstance(cand_features[key], dict):\n",
        "        cand_features[key] = {}\n",
        "    return cand_features[key]\n",
        "\n",
        "\n",
        "def build_edges(graph, rng, p_zero_abdr=0.02, p_prev_xm=0.05):\n",
        "    \"\"\"\n",
        "    E(t) = ABOCompatible(V(t)) and weight = w_optn(u,v)\n",
        "    Also fill match-specific dict flags per donor id for w_optn.\n",
        "    \"\"\"\n",
        "    graph.E.clear()\n",
        "    vids = list(graph.V.keys())\n",
        "\n",
        "    for i in vids:\n",
        "        u = graph.V[i]\n",
        "        try:\n",
        "            d_abo = donor_abo_of(u)\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "        for j in vids:\n",
        "            if i == j:\n",
        "                continue\n",
        "            v = graph.V[j]\n",
        "            if v.is_altruist:\n",
        "                continue  # altruists cannot receive\n",
        "\n",
        "            try:\n",
        "                c_abo = candidate_abo_of(v)\n",
        "            except KeyError:\n",
        "                continue\n",
        "\n",
        "            if not abo_compatible(d_abo, c_abo):\n",
        "                continue\n",
        "\n",
        "            # match-specific features for this donor u\n",
        "            z_dict = _ensure_dict_feature(v.features, \"zero_abdr_mismatch\")\n",
        "            px_dict = _ensure_dict_feature(v.features, \"prev_crossmatch_ok\")\n",
        "            if u.id not in z_dict:\n",
        "                z_dict[u.id] = (rng.random() < p_zero_abdr)\n",
        "            if u.id not in px_dict:\n",
        "                px_dict[u.id] = (rng.random() < p_prev_xm)\n",
        "\n",
        "            graph.E[(i, j)] = w_optn(u, v)\n",
        "\n",
        "def _matching_to_succ_pred(matching):\n",
        "    succ, pred = {}, {}\n",
        "    for u, v in matching:\n",
        "        succ[u] = v\n",
        "        pred[v] = u\n",
        "    return succ, pred\n",
        "\n",
        "\n",
        "def _find_cycles_from_succ(succ):\n",
        "    \"\"\"\n",
        "    Since outdegree<=1 and indegree<=1, each component is either a simple path or a simple cycle.\n",
        "    A cycle here must return to its start.\n",
        "    Returns list of cycles, each cycle is list of vertex ids in cycle order.\n",
        "    \"\"\"\n",
        "    visited = set()\n",
        "    cycles = []\n",
        "\n",
        "    for start in succ.keys():\n",
        "        if start in visited:\n",
        "            continue\n",
        "        cur = start\n",
        "        path = []\n",
        "        seen = set()\n",
        "        while cur in succ and cur not in seen:\n",
        "            seen.add(cur)\n",
        "            path.append(cur)\n",
        "            cur = succ[cur]\n",
        "        # if we returned to start, it's a cycle\n",
        "        if cur == start:\n",
        "            cycles.append(path)\n",
        "\n",
        "        visited.update(path)\n",
        "\n",
        "    return cycles\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def step_pool(graph, t, lam_p, lam_a, f_p, f_a, rng,\n",
        "              expire_prob=0.01, renege_prob=0.02,\n",
        "              max_cycle_len=3, max_chain_len=4):\n",
        "    # 1) SolveIP\n",
        "    matching_edges = SolveIP(graph, max_cycle_len=max_cycle_len, max_chain_len=max_chain_len)\n",
        "\n",
        "    # 2) Expire on V(t)\n",
        "    expired = set()\n",
        "    for vid, v in list(graph.V.items()):\n",
        "        if expire(v, rng, prob=expire_prob):\n",
        "            expired.add(vid)\n",
        "\n",
        "    # remove expired vertices from matching\n",
        "    matching_edges = [(u, v) for (u, v) in matching_edges if (u not in expired and v not in expired)]\n",
        "    succ, pred = _matching_to_succ_pred(matching_edges)\n",
        "\n",
        "    departures = set(expired)\n",
        "\n",
        "    # 3) Cycles: all-or-nothing\n",
        "    cycles = _find_cycles_from_succ(succ)\n",
        "    for cyc in cycles:\n",
        "        # cycles should contain only pairs; if not, skip\n",
        "        if any(graph.V[x].is_altruist for x in cyc):\n",
        "            continue\n",
        "\n",
        "        ok = True\n",
        "        for v_id in cyc:\n",
        "            if negative_crossmatch(graph.V[v_id], rng):\n",
        "                ok = False\n",
        "                break\n",
        "\n",
        "        if ok:\n",
        "            # all vertices in a successful cycle depart\n",
        "            for v_id in cyc:\n",
        "                departures.add(v_id)\n",
        "\n",
        "    # 4) Chains: sequential with tail cut\n",
        "    chain_starts = [u for u in succ if graph.V[u].is_altruist]\n",
        "    for start in chain_starts:\n",
        "        cur = start\n",
        "        chain_executed = False\n",
        "\n",
        "        while cur in succ:\n",
        "            nxt = succ[cur]\n",
        "            if nxt not in graph.V or graph.V[nxt].is_altruist:\n",
        "                break\n",
        "\n",
        "            # recipient crossmatch fail => stop, recipient doesn't depart\n",
        "            if negative_crossmatch(graph.V[nxt], rng):\n",
        "                break\n",
        "\n",
        "            # first successful transplant into nxt\n",
        "            chain_executed = True\n",
        "            departures.add(nxt)\n",
        "\n",
        "            # if nxt reneges, stop here (tail removed)\n",
        "            if renege(graph.V[nxt], rng, default_prob=renege_prob):\n",
        "                break\n",
        "\n",
        "            cur = nxt\n",
        "\n",
        "        if chain_executed:\n",
        "            departures.add(start)  # altruist departs only if chain started\n",
        "\n",
        "    # IMPORTANT: after failures, some matched vertices might remain in succ/pred\n",
        "    # but since we're ignoring OWQ we simply update pool by departures.\n",
        "\n",
        "    # 5) Remove departures\n",
        "    for vid in departures:\n",
        "        graph.V.pop(vid, None)\n",
        "\n",
        "    # 6) New arrivals + rebuild edges/weights\n",
        "    sample_arrivals(t, graph, lam_p, lam_a, f_p, f_a, rng)\n",
        "    build_edges(graph, rng)\n",
        "\n",
        "    return graph, departures\n"
      ],
      "metadata": {
        "id": "JHKn54sBceq7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Demo"
      ],
      "metadata": {
        "id": "MAoG464AJyxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    setpool = step_pool\n",
        "    rng = np.random.default_rng(0)\n",
        "    g = Graph()\n",
        "\n",
        "    # Pair records must include: candidate_abo, paired_donor_abo, cpra, wait_days\n",
        "    pair_bank = [\n",
        "        {\n",
        "            \"type\": \"pair\",\n",
        "            \"candidate_abo\": \"B\",\n",
        "            \"paired_donor_abo\": \"A\",\n",
        "            \"cpra\": 20,\n",
        "            \"wait_days\": 60,\n",
        "            \"candidate_age\": 45,\n",
        "            \"prior_living_donor\": False,\n",
        "            \"orphan\": False,\n",
        "            \"center\": 1,\n",
        "            \"zero_abdr_mismatch\": {},\n",
        "            \"prev_crossmatch_ok\": {},\n",
        "            \"renege_prob\": 0.02,\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"pair\",\n",
        "            \"candidate_abo\": \"O\",\n",
        "            \"paired_donor_abo\": \"B\",\n",
        "            \"cpra\": 80,\n",
        "            \"wait_days\": 200,\n",
        "            \"candidate_age\": 30,\n",
        "            \"prior_living_donor\": False,\n",
        "            \"orphan\": False,\n",
        "            \"center\": 2,\n",
        "            \"zero_abdr_mismatch\": {},\n",
        "            \"prev_crossmatch_ok\": {},\n",
        "            \"renege_prob\": 0.02,\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"pair\",\n",
        "            \"candidate_abo\": \"AB\",\n",
        "            \"paired_donor_abo\": \"O\",\n",
        "            \"cpra\": 95,\n",
        "            \"wait_days\": 10,\n",
        "            \"candidate_age\": 12,\n",
        "            \"prior_living_donor\": False,\n",
        "            \"orphan\": False,\n",
        "            \"center\": 1,\n",
        "            \"zero_abdr_mismatch\": {},\n",
        "            \"prev_crossmatch_ok\": {},\n",
        "            \"renege_prob\": 0.02,\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    altruist_bank = [\n",
        "        {\"type\": \"altruist\", \"donor_abo\": \"O\", \"center\": 1},\n",
        "        {\"type\": \"altruist\", \"donor_abo\": \"A\", \"center\": 2},\n",
        "        {\"type\": \"altruist\", \"donor_abo\": \"B\", \"center\": 3},\n",
        "    ]\n",
        "\n",
        "    f_p = EmpiricalSampler(pair_bank, rng)\n",
        "    f_a = EmpiricalSampler(altruist_bank, rng)\n",
        "\n",
        "    sample_arrivals(0, g, lam_p=3.0, lam_a=1.0, f_p=f_p, f_a=f_a, rng=rng)\n",
        "    build_edges(g, rng)\n",
        "\n",
        "    print(\"Initial: |V(0)| =\", len(g.V), \", |E(0)| =\", len(g.E))\n",
        "\n",
        "    for t in range(5):\n",
        "        g, D = step_pool(\n",
        "            graph=g,\n",
        "            t=t,\n",
        "            lam_p=3.0,\n",
        "            lam_a=1.0,\n",
        "            f_p=f_p,\n",
        "            f_a=f_a,\n",
        "            rng=rng,\n",
        "            expire_prob=0.01,\n",
        "            renege_prob=0.02,\n",
        "        )\n",
        "        print(\n",
        "            \"t =\", t + 1,\n",
        "            \"|departures| =\", len(D),\n",
        "            \", |V(t)| =\", len(g.V),\n",
        "            \", |E(t)| =\", len(g.E),\n",
        "        )\n"
      ],
      "metadata": {
        "id": "76ZFo2mMJqqK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8237d5c-da04-45e8-c022-469ed5e900c1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial: |V(0)| = 2 , |E(0)| = 2\n",
            "t = 1 |departures| = 0 , |V(t)| = 4 , |E(t)| = 7\n",
            "t = 2 |departures| = 2 , |V(t)| = 8 , |E(t)| = 36\n",
            "t = 3 |departures| = 0 , |V(t)| = 12 , |E(t)| = 80\n",
            "t = 4 |departures| = 2 , |V(t)| = 14 , |E(t)| = 126\n",
            "t = 5 |departures| = 0 , |V(t)| = 17 , |E(t)| = 184\n"
          ]
        }
      ]
    }
  ]
}